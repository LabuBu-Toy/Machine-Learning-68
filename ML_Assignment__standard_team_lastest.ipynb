{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LabuBu-Toy/Machine-Learning-68/blob/main/ML_Assignment__standard_team_lastest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w24HpG8cbRIT"
      },
      "source": [
        "# Fetal-health-classification üë∂üèªüê£üçº\n",
        "\n",
        "Dataset Link: https://www.kaggle.com/datasets/andrewmvd/fetal-health-classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0c5bff5"
      },
      "source": [
        "!pip install imblearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0m2vADVgstL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display\n",
        "from pprint import pprint\n",
        "from fnmatch import fnmatchcase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brdEfLXNFcJn"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore') # Ignore any warning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itSPugBCg2t8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report,  log_loss,\n",
        "    confusion_matrix, ConfusionMatrixDisplay, recall_score,\n",
        "    balanced_accuracy_score, make_scorer, precision_score,\n",
        "    f1_score, roc_auc_score, average_precision_score)\n",
        "\n",
        "from imblearn.metrics import classification_report_imbalanced\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eldeIViff5Lw"
      },
      "outputs": [],
      "source": [
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, SVMSMOTE, SMOTEN, SMOTENC, KMeansSMOTE\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.under_sampling import TomekLinks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_PC3FM6gqag"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.calibration import CalibratedClassifierCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4FZm-kMQro-"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import clone\n",
        "from fnmatch import fnmatchcase\n",
        "from collections import Counter, defaultdict\n",
        "from math import ceil\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Optional, Iterable\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.naive_bayes import CategoricalNB\n",
        "from sklearn.model_selection import GridSearchCV, HalvingGridSearchCV\n",
        "\n",
        "from imblearn.over_sampling import (\n",
        "    RandomOverSampler, SMOTE, BorderlineSMOTE, SVMSMOTE,\n",
        "    SMOTEN, SMOTENC, KMeansSMOTE\n",
        ")\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier, RUSBoostClassifier"
      ],
      "metadata": {
        "id": "EFDY98OGLIk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-73R9m1JF1Jb"
      },
      "outputs": [],
      "source": [
        "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
        "from sklearn.model_selection import HalvingRandomSearchCV\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNJt5CWthElC"
      },
      "outputs": [],
      "source": [
        "sns.set(style='darkgrid')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, f1_score, balanced_accuracy_score, confusion_matrix\n",
        "from scipy.stats import randint, loguniform"
      ],
      "metadata": {
        "id": "kllz3VX5LPiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cf28Jp0OYbcJ"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "from pathlib import Path\n",
        "\n",
        "# Download Dataset from Kaggle\n",
        "dataset_path = kagglehub.dataset_download(\"andrewmvd/fetal-health-classification\")\n",
        "\n",
        "file_path = Path(dataset_path)\n",
        "file_path = file_path / 'fetal_health.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYTUOgUxowME"
      },
      "outputs": [],
      "source": [
        "# Set Random SEED\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wn9kI1vqGSB"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(file_path)\n",
        "display(data)\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPJQ8ABaB2Lo"
      },
      "source": [
        "## Data Cleaning & preparation\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"‡∏Ç‡∏ô‡∏≤‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏ö:\", data.shape)\n",
        "data = data.drop_duplicates(keep=\"first\")\n",
        "print(\"‡∏Ç‡∏ô‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏á‡∏•‡∏ö:\", data.shape)\n"
      ],
      "metadata": {
        "id": "b6EU9rLLiQqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Missing Values"
      ],
      "metadata": {
        "id": "I7gnsJWVLm5s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oI92BS1ASeE"
      },
      "outputs": [],
      "source": [
        "pprint(f\"Data Shape: {data.shape}\")\n",
        "pprint(f\"Missing values: {data.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Percentage per class\n",
        "pprint(data['fetal_health'].value_counts(normalize=True, sort=True)\n",
        "     .mul(100).round(4))"
      ],
      "metadata": {
        "id": "xlTJDRZds4s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZp1B9FRx1-B"
      },
      "outputs": [],
      "source": [
        "# Class distribution\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.countplot(x='fetal_health', data=data, palette='Greens')\n",
        "plt.title('Fetal Health Class Distribution')\n",
        "plt.xlabel('Class (1: Normal, 2: Suspect, 3: Pathological)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XHAGseuQaNG"
      },
      "outputs": [],
      "source": [
        "# Counts per class (sorted by feature label)\n",
        "pprint(data['fetal_health'].value_counts(sort=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp50bExpRkEM"
      },
      "outputs": [],
      "source": [
        "# Percentage per class\n",
        "pprint(data['fetal_health'].value_counts(normalize=True, sort=True)\n",
        "     .mul(100).round(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWKMig8k87vL"
      },
      "source": [
        "\n",
        "> Dataset are Imbalance\n",
        "\n",
        "#### When if data imbalance What will goes wrong\n",
        "\n",
        "- Skewed decision boundary: Model predicts the majority most of the time ‚Üí low recall for minority.\n",
        "\n",
        "- Misleading metrics: Overall accuracy/micro-F1 look high even if minority recall ‚âà 0. ROC-AUC can look okay; PR (precision‚Äìrecall) is more honest.\n",
        "\n",
        "- Unstable training: Few minority examples ‚Üí high variance, overfitting (especially if you duplicate them).\n",
        "\n",
        "- Poor calibration: Probabilities get biased toward the majority prior.\n",
        "\n",
        "> Tiny example (990 zeros, 10 ones): always predicting ‚Äú0‚Äù gives 99% accuracy, but minority recall = 0%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw7QPYdIQFRU"
      },
      "source": [
        "\n",
        "> **Baseline accuracy:** the share of the majority class. If class 1 is 70%, a naive \"always‚Äë1\" model gets 70% accuracy ‚Äîwhy accuracy alone can mislead.\n",
        "\n",
        "---\n",
        "\n",
        "### What to do if it‚Äôs class imbalanced\n",
        "\n",
        "* **Metrics:** use **F1 (macro/weighted)**, **recall** for critical classes (e.g., Pathological), **PR‚ÄëAUC**.\n",
        "* **Class weights:**  Pass Class weights to models that support `class_weight`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKCLKzSHShpl"
      },
      "source": [
        "\n",
        "\n",
        "* **Resampling:** oversample minority (e.g., **SMOTE**) or undersample majority (use carefully to avoid losing signal).\n",
        "* **Stratify splits:** Strategic\n",
        "* **Threshold tuning:** Adjust decision thresholds to trade precision vs recall per class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brMH4J3YScsB"
      },
      "source": [
        "### Common pitfalls\n",
        "\n",
        "* **Accuracy paradox:** High accuracy from predicting only the majority class.\n",
        "* **Data leakage:** Balancing after splitting can leak information‚Äîapply resampling **within** cross‚Äëvalidation folds.\n",
        "* **Over‚Äësynthetic patterns:** Aggressive SMOTE settings can overfit; monitor with validation curves.\n",
        "\n",
        "---\n",
        "\n",
        "> TL-DR: Class distribution = \"**How many (or what percent) of each label do we have?**\"\n",
        "Check it early. If it‚Äôs skewed, choose proper metrics, stratify, consider class weights/resampling, and tune thresholds."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distribution of Key Variables\n",
        "\n",
        "We analyzed histograms to understand the distribution of critical variables and their relationships with the target variable."
      ],
      "metadata": {
        "id": "i-A4E6spR2M-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all feature columns except the target variable\n",
        "feature_columns = data.columns.drop('fetal_health')\n",
        "\n",
        "# Plot histograms for all feature columns, colored by fetal_health\n",
        "for feature in feature_columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(data=data, x=feature, hue='fetal_health', multiple='stack', kde=True, palette='viridis')\n",
        "    plt.title(f'Distribution of {feature} by Fetal Health')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Count')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "DEyaO9O8R-eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqihg2HsR-mH"
      },
      "source": [
        "## Data Correlation Heatmap\n",
        "\n",
        "A correlation heatmap is a color-coded grid that shows the pairwise correlation coefficients between variables in a dataset. Each row/column is a variable; each cell‚Äôs color and number indicate how strongly two variables move together:\n",
        "\n",
        "Value range: from ‚Äì1 to +1\n",
        "\n",
        "- +1: perfect positive linear relationship (as one increases, the other increases)\n",
        "\n",
        "- 0: no linear relationship\n",
        "\n",
        "- ‚Äì1: perfect negative linear relationship (as one increases, the other decreases)\n",
        "\n",
        "> Symmetry: the matrix is symmetric; the diagonal is always 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzK5JiAudDrQ"
      },
      "outputs": [],
      "source": [
        "# Correlation heatmap pearson method\n",
        "plt.figure(figsize=(15, 12))\n",
        "sns.heatmap(data.corr(method=\"pearson\"), annot=True, fmt='.2f', cmap=\"coolwarm\", square=True) #use\n",
        "plt.title('Feature Correlation Heatmap pearson method')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INpAwNc9y1As"
      },
      "outputs": [],
      "source": [
        "# Correlation heatmap Spearman\n",
        "plt.figure(figsize=(15, 12))\n",
        "sns.heatmap(data.corr(method=\"spearman\"), annot=True, fmt='.2f', cmap=\"coolwarm\", square=True) #use\n",
        "plt.title('Feature Correlation Heatmap spearman method')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw8DE9ySd9Ua"
      },
      "source": [
        "# Step 3: Data Prepareation\n",
        "\n",
        "Separate features and target variable.**bold text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e_-Bp_zQXEH"
      },
      "outputs": [],
      "source": [
        "# Features / Target\n",
        "X = data.drop(columns=['fetal_health']).select_dtypes(include=[np.number]).copy()   # explicit list + copy for safety\n",
        "\n",
        "# Optional: drop all columns starting with \"histogram\" (case-insensitive)\n",
        "# X = X.loc[:, ~X.columns.str.lower().str.startswith('histogram')]\n",
        "\n",
        "# Kaggle labels are floats (1.0/2.0/3.0); casting to int is often nicer\n",
        "y = data['fetal_health'].astype('int64')\n",
        "\n",
        "# 2) Check Data Shape\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "assert 'fetal_health' not in X.columns\n",
        "# assert not any(X.columns.str.lower().str.startswith('histogram'))  # if you dropped them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO21WcCwEIVp"
      },
      "source": [
        "# Step 4: Data Train-Test-Split and k-Fold Cross Validation\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,          # preserves class proportions in each split\n",
        "    random_state=SEED\n",
        ")"
      ],
      "metadata": {
        "id": "Z7KN8rEvyGfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM6TVUEDDOCk"
      },
      "source": [
        "# Step 5: Models Training, Tuning and Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dt2ic06y6Sv"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdTMU8uEJFEl"
      },
      "outputs": [],
      "source": [
        "# ======== CONFIG ========\n",
        "smote_like = (SMOTE, BorderlineSMOTE, SVMSMOTE, SMOTEN, SMOTENC, KMeansSMOTE, SMOTEENN, SMOTETomek, TomekLinks)\n",
        "\n",
        "PRIMARY_SCORER = \"f1_macro\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeeNUY1Ty4fA"
      },
      "outputs": [],
      "source": [
        "# Metrics:  Accuracy, Precision, Recall, plus your F1s\n",
        "# NOTE: log-loss scorer in sklearn is named \"neg_log_loss\" (maximized),\n",
        "# so we expose it as \"log_loss\" and flip the sign when reporting.\n",
        "scoring = {\n",
        "    \"f1_macro\": \"f1_macro\",\n",
        "    \"f1_weighted\": \"f1_weighted\",\n",
        "    \"accuracy\": \"accuracy\",\n",
        "    \"balanced_acc\": \"balanced_accuracy\",\n",
        "    \"precision_macro\": \"precision_macro\",\n",
        "    \"precision_weighted\": \"precision_weighted\",\n",
        "    \"recall_macro\": \"recall_macro\",\n",
        "    \"recall_weighted\": \"recall_weighted\",\n",
        "\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND2RP1E2IoHy"
      },
      "source": [
        "## Function Helper\n",
        "\n",
        "Define a helper function for training and evaluating models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zii8qhrkInyg"
      },
      "outputs": [],
      "source": [
        "def grid_for(model_name: str, grid_map: dict) -> dict | None:\n",
        "    # allow both exact and wildcard matches; also fix \"LogReg_*\" -> \"LogReg*\"\n",
        "    grid_map = {k.replace(\"_*\", \"*\"): v for k, v in grid_map.items()}\n",
        "    if model_name in grid_map:\n",
        "        return grid_map[model_name]\n",
        "    for pattern, grid in grid_map.items():\n",
        "        if fnmatchcase(model_name, pattern):\n",
        "            return grid\n",
        "    return None\n",
        "\n",
        "def find_smote_param_prefixes(estimator):\n",
        "    \"\"\"Return param prefixes pointing to SMOTE-like instances (supports nesting, e.g. 'sampler__smote').\"\"\"\n",
        "    prefixes = []\n",
        "    for k, v in estimator.get_params(deep=True).items():\n",
        "        if isinstance(v, smote_like):\n",
        "            prefixes.append(k)\n",
        "    return prefixes\n",
        "\n",
        "def safe_k_neighbors(y, n_splits, default_k=5):\n",
        "    \"\"\"Safe SMOTE k per worst-case train fold.\"\"\"\n",
        "    counts = Counter(y)\n",
        "    min_train = min(c - ceil(c / n_splits) for c in counts.values())\n",
        "    if min_train < 2:\n",
        "        return None\n",
        "    return max(1, min(default_k, min_train - 1))\n",
        "\n",
        "def pipeline_has_smote_or_smoteenn(estimator) -> bool:\n",
        "    \"\"\"Return True if any parameter in the estimator tree is an instance of a SMOTE-family sampler or SMOTEENN.\"\"\"\n",
        "    for _, v in estimator.get_params(deep=True).items():\n",
        "        if isinstance(v, smote_like):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def evaluate_on_test(est, X_test = X_test, y_test = y_test):\n",
        "    \"\"\"Return dict of test metrics and show confusion matrices.\"\"\"\n",
        "\n",
        "    y_pred = est.predict(X_test)\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"balanced_acc\": balanced_accuracy_score(y_test, y_pred),\n",
        "        \"precision_macro\": precision_score(y_test, y_pred, average=\"macro\", zero_division=0),\n",
        "        \"precision_weighted\": precision_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
        "        \"recall_macro\": recall_score(y_test, y_pred, average=\"macro\", zero_division=0),\n",
        "        \"recall_weighted\": recall_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
        "        \"f1_macro\": f1_score(y_test, y_pred, average=\"macro\"),\n",
        "        \"f1_weighted\": f1_score(y_test, y_pred, average=\"weighted\"),\n",
        "    }\n",
        "    # log loss & ROC-AUC if proba/decision available\n",
        "    if hasattr(est, \"predict_proba\"):\n",
        "        y_prob = est.predict_proba(X_test)\n",
        "        try:\n",
        "            metrics[\"log_loss\"] = log_loss(y_test, y_prob, labels=np.unique(y_test))\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            metrics[\"roc_auc_ovr\"] = roc_auc_score(y_test, y_prob, multi_class=\"ovr\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    print(\"#\" * 20)\n",
        "\n",
        "    print(\"\\n- Imbalance Classification report (test):\")\n",
        "    print(classification_report_imbalanced(y_test, y_pred, digits=4, zero_division=0))\n",
        "\n",
        "    # Confusion matrices\n",
        "    try:\n",
        "        ConfusionMatrixDisplay.from_estimator(est, X_test, y_test)\n",
        "        plt.title(\"Confusion Matrix (counts)\")\n",
        "        plt.show()\n",
        "        ConfusionMatrixDisplay.from_estimator(est, X_test, y_test, normalize=\"true\")\n",
        "        plt.title(\"Confusion Matrix (normalized)\")\n",
        "        plt.show()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    print(\"#\" * 20)\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7Tq9pTWzFEq"
      },
      "outputs": [],
      "source": [
        "pipelines = {\n",
        "    \"RF_balancedRF\": ImbPipeline([\n",
        "        (\"sel\", SelectKBest(score_func=f_classif)), # supervised selection\n",
        "        (\"clf\", BalancedRandomForestClassifier(\n",
        "            bootstrap=True,\n",
        "            random_state=SEED\n",
        "        ))\n",
        "    ]),\n",
        "    \"RF_pure\" : ImbPipeline([\n",
        "      (\"clf\", RandomForestClassifier(\n",
        "          bootstrap=True,\n",
        "          oob_score=False,\n",
        "          random_state=SEED))\n",
        "    ]),\n",
        "    \"RF_class_weighted\" : ImbPipeline([\n",
        "      (\"clf\", RandomForestClassifier(\n",
        "          bootstrap=True,\n",
        "          oob_score=False,\n",
        "          class_weight=\"balanced_subsample\",\n",
        "          random_state=SEED))\n",
        "    ]),\n",
        "    \"RF_FS\": ImbPipeline([\n",
        "        (\"sel\", SelectKBest(score_func=f_classif)), # supervised selection\n",
        "        (\"clf\", RandomForestClassifier(\n",
        "            bootstrap=True,\n",
        "            class_weight=\"balanced_subsample\",\n",
        "            random_state=SEED\n",
        "        ))\n",
        "    ]),\n",
        "    \"RF_BorderlineSMOTE\": ImbPipeline([\n",
        "        (\"scale\", StandardScaler()),\n",
        "        (\"sel\", SelectKBest(score_func=f_classif)), # supervised selection\n",
        "        (\"borderline_smote\", BorderlineSMOTE(random_state=SEED)),\n",
        "        (\"clf\", RandomForestClassifier(\n",
        "            bootstrap=True,\n",
        "            random_state=SEED))\n",
        "    ]),\n",
        "\n",
        "    \"RF_SMOTE\": ImbPipeline([\n",
        "        (\"scale\", StandardScaler()),\n",
        "        (\"sel\", SelectKBest(score_func=f_classif)), # supervised selection\n",
        "        (\"smote\", SMOTE(random_state=SEED)),\n",
        "        (\"clf\", RandomForestClassifier(\n",
        "            bootstrap=True,\n",
        "            random_state=SEED))\n",
        "    ]),\n",
        "    \"RF_SMOTETomek\": ImbPipeline([\n",
        "        (\"scale\", StandardScaler()),\n",
        "        (\"sel\", SelectKBest(score_func=f_classif)), # supervised selection\n",
        "        (\"smtomek\", SMOTETomek(random_state=SEED)),\n",
        "        (\"clf\", RandomForestClassifier(\n",
        "            bootstrap=True,\n",
        "            random_state=SEED))\n",
        "    ])\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sYH40GCF6jH"
      },
      "source": [
        "## Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFzN0GJzIi88"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import randint, loguniform, uniform\n",
        "\n",
        "search_spaces = {\n",
        "    # === Baselines ===\n",
        "    \"RF_balancedRF\": {\n",
        "        \"sel\": [SelectKBest(score_func=f_classif, k=10), SelectKBest(score_func=f_classif, k=20)],\n",
        "        \"clf__n_estimators\": [300, 600, 1000],\n",
        "        \"clf__max_depth\": [None, 10, 20, 30],\n",
        "        \"clf__min_samples_leaf\": [1, 2, 4],\n",
        "        \"clf__max_features\": [\"sqrt\", 0.5, 0.8],\n",
        "        \"clf__n_jobs\": [-1],\n",
        "    },\n",
        "    \"RF_pure\": {\n",
        "        \"clf__n_estimators\": [300, 600, 1000],\n",
        "        \"clf__max_depth\": [None, 10, 20, 30],\n",
        "        \"clf__min_samples_leaf\": [1, 2, 4],\n",
        "        \"clf__max_features\": [\"sqrt\", 0.5, 0.8],\n",
        "        \"clf__n_jobs\": [-1],\n",
        "    },\n",
        "    \"RF_class_weighted\": {\n",
        "        \"sel\": [SelectKBest(score_func=f_classif, k=10), SelectKBest(score_func=f_classif, k=20)],\n",
        "        \"clf__n_estimators\": [300, 600, 1000],\n",
        "        \"clf__max_depth\": [None, 10, 20, 30],\n",
        "        \"clf__min_samples_leaf\": [1, 2, 4],\n",
        "        \"clf__max_features\": [\"sqrt\", 0.5, 0.8],\n",
        "        \"clf__n_jobs\": [-1],\n",
        "    },\n",
        "    \"RF_FS\" : {\n",
        "        \"sel\": [SelectKBest(score_func=f_classif), SelectKBest(score_func=f_classif, k=20)],\n",
        "        \"clf__n_estimators\": [300, 600, 1000],\n",
        "        \"clf__max_depth\": [None, 10, 20, 30],\n",
        "        \"clf__min_samples_leaf\": [1, 2, 4],\n",
        "        \"clf__max_features\": [\"sqrt\", 0.5, 0.8],\n",
        "        \"clf__n_jobs\": [-1],\n",
        "    },\n",
        "\n",
        "    # === Samplers ===\n",
        "    \"RF_SMOTE\": {\n",
        "        \"sel\": [SelectKBest(score_func=f_classif, k=10), SelectKBest(score_func=f_classif, k=20)],\n",
        "        \"smote__k_neighbors\": [3, 5, 7],                                   # SMOTE uses k-NN\n",
        "        \"smote__sampling_strategy\": [\"not majority\", \"all\"],               # strings for multiclass\n",
        "        \"clf__n_estimators\": [300, 600, 1000],\n",
        "        \"clf__max_depth\": [None, 10, 20],\n",
        "        \"clf__min_samples_leaf\": [1, 2, 4],\n",
        "        \"clf__max_features\": [\"sqrt\", 0.5],\n",
        "        \"clf__n_jobs\": [-1],\n",
        "    },\n",
        "    \"RF_BorderlineSMOTE\": {\n",
        "        \"sel\": [SelectKBest(score_func=f_classif, k=10), SelectKBest(score_func=f_classif, k=20)],\n",
        "        \"borderline_smote__k_neighbors\": [3, 5],\n",
        "        \"borderline_smote__m_neighbors\": [5, 10],\n",
        "        \"borderline_smote__kind\": [\"borderline-1\", \"borderline-2\"],\n",
        "        \"clf__n_estimators\": [300, 600, 1000],\n",
        "        \"clf__max_depth\": [None, 10, 20],\n",
        "        \"clf__min_samples_leaf\": [1, 2, 4],\n",
        "        \"clf__max_features\": [\"sqrt\", 0.5],\n",
        "        \"clf__n_jobs\": [-1],\n",
        "    },\n",
        "    \"RF_SMOTETomek\": {\n",
        "        \"sel\": [SelectKBest(score_func=f_classif, k=10), SelectKBest(score_func=f_classif, k=20)],\n",
        "        \"smtomek__smote__k_neighbors\": [3, 5],                             # nested estimator\n",
        "        \"smtomek__sampling_strategy\": [\"not majority\", \"all\"],\n",
        "        \"clf__n_estimators\": [300, 600, 1000],\n",
        "        \"clf__max_depth\": [None, 10, 20],\n",
        "        \"clf__min_samples_leaf\": [1, 2, 4],\n",
        "        \"clf__max_features\": [\"sqrt\", 0.5],\n",
        "        \"clf__n_jobs\": [-1],\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMrOksAwz5YM"
      },
      "source": [
        "## Cross Validation Process\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wF_95XSopluQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H78ND-o3iN0"
      },
      "source": [
        "## Skip tune model that use SMOTE and their families"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bsh45o5I3hKS"
      },
      "outputs": [],
      "source": [
        "# --- build the filtered dict + a small report ---\n",
        "skipped, pipelines_to_tune = [], {}\n",
        "for name, pipe in pipelines.items():\n",
        "    if pipeline_has_smote_or_smoteenn(pipe):\n",
        "        skipped.append(name)\n",
        "    else:\n",
        "        pipelines_to_tune[name] = pipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xphsqbkJ3o9U"
      },
      "source": [
        "## Tuning Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqsVTqIj3veO"
      },
      "outputs": [],
      "source": [
        "print(\"[TUNE SELECTED] Will tune these pipelines: \", end=\"\\t\")\n",
        "pprint(pipelines_to_tune.keys())\n",
        "\n",
        "if len(skipped) > 0:\n",
        "    print(\"[TUNE SKIPED] Skipping pipelines using SMOTE-Families: \", end=\"\\t\")\n",
        "    pprint(skipped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "I5UKFWvUL9mV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "c7caee22-06c5-4723-8f55-10d8133dd8dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] n_splits=100\n",
            "[TUNE] RF_balancedRF with params: ['sel', 'clf__n_estimators', 'clf__max_depth', 'clf__min_samples_leaf', 'clf__max_features', 'clf__n_jobs']\n",
            "n_iterations: 1\n",
            "n_required_iterations: 1\n",
            "n_possible_iterations: 1\n",
            "min_resources_: 600\n",
            "max_resources_: 1690\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 2\n",
            "n_resources: 600\n",
            "Fitting 100 folds for each of 2 candidates, totalling 200 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3778481176.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Perform the hyperparameter search by fitting the search object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Store the tuning results and the best estimator directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search_successive_halving.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_samples_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;31m# Set best_score_: BaseSearchCV does not set it, as refit is a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search_successive_halving.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m    355\u001b[0m             }\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             results = evaluate_candidates(\n\u001b[0m\u001b[1;32m    358\u001b[0m                 \u001b[0mcandidate_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmore_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmore_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "n_splits = getattr(cv, \"n_splits\", None) or cv.get_n_splits(X_train, y_train)\n",
        "print(f\"[CV] n_splits={n_splits}\")\n",
        "\n",
        "model_tuned: dict[str, dict] = defaultdict(dict)\n",
        "\n",
        "for model_name, base_est in pipelines_to_tune.items():\n",
        "    grid = grid_for(model_name, search_spaces)\n",
        "    if not grid:\n",
        "        print(f\"[SKIP] {model_name}: no matching search space.\")\n",
        "        continue\n",
        "\n",
        "    est = clone(base_est)\n",
        "\n",
        "    print(f\"[TUNE] {model_name} with params: {list(grid.keys()) or '[no tunable params]'}\")\n",
        "\n",
        "    search = HalvingRandomSearchCV(\n",
        "        estimator=est,\n",
        "        param_distributions=grid,\n",
        "        scoring=PRIMARY_SCORER,\n",
        "        cv=cv,\n",
        "        factor=3,\n",
        "        random_state=SEED,\n",
        "        n_jobs=-1,\n",
        "        error_score=np.nan,   # don't bomb entire run on a single bad fold\n",
        "        refit=\"f1_macro\",\n",
        "        verbose=1,\n",
        "    )\n",
        "\n",
        "    # Perform the hyperparameter search by fitting the search object\n",
        "    search.fit(X_train, y_train)\n",
        "\n",
        "    # Store the tuning results and the best estimator directly\n",
        "    model_tuned[model_name][\"tune_results\"] = {\n",
        "        \"best_score\": float(search.best_score_),\n",
        "        \"best_params\": search.best_params_,\n",
        "        \"n_candidates\": getattr(search, \"n_candidates_\", None),\n",
        "        \"n_iterations\": getattr(search, \"n_iterations_\", None),\n",
        "    }\n",
        "    model_tuned[model_name][\"tuned_estimators\"] = search.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report Models"
      ],
      "metadata": {
        "id": "EYzZb2163UWy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CI4XjG65Zod"
      },
      "source": [
        "## Summary Tune Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsTzKJTa5ci0"
      },
      "outputs": [],
      "source": [
        "# Summary table\n",
        "summary_df = (\n",
        "    pd.DataFrame([\n",
        "        {\"model\": name, **vals[\"tune_results\"]} for name, vals in model_tuned.items()\n",
        "    ]).sort_values(\"best_score\", ascending=False).reset_index(drop=True)\n",
        ")\n",
        "# Rename the 'best_score' column to be more explicit\n",
        "summary_df.rename(columns={\"best_score\": \"best_f1_macro_CV_score\"}, inplace=True)\n",
        "\n",
        "display(summary_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h8yE_7MNEBH"
      },
      "source": [
        "## Imbalance Classification report of Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dLhg7-BJ_7S"
      },
      "outputs": [],
      "source": [
        "# Evaluate all tuned models on the test set\n",
        "print(\"Evaluating all tuned models on the test set:\")\n",
        "\n",
        "tuned_models_list = [name for name, vals in model_tuned.items() if \"best_score\" in vals.get(\"tune_results\", {})]\n",
        "\n",
        "if not tuned_models_list:\n",
        "    print(\"No tuned models found in model_tune dictionary.\")\n",
        "else:\n",
        "    for model_name in tuned_models_list:\n",
        "        print(f\"\\nEvaluating tuned model: {model_name}\")\n",
        "        # Access the fitted tuned estimator\n",
        "        tuned_estimator = model_tuned[model_name][\"tuned_estimators\"]\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        test_metrics = evaluate_on_test(tuned_estimator, X_test, y_test)\n",
        "\n",
        "        # Display scores for the tuned model in a table\n",
        "        print(f\"\\nSummary of Scores for Tuned Model: {model_name} on Test Set:\")\n",
        "        tuned_scores_df = pd.DataFrame([test_metrics])\n",
        "        display(tuned_scores_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSnbt_R1NMY6"
      },
      "source": [
        "## Train untuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaPdbEQKGA64"
      },
      "outputs": [],
      "source": [
        "# Evaluate all original, untuned models on the test set for comparison\n",
        "print(\"Evaluating original, untuned models:\")\n",
        "print(model_tuned.keys())\n",
        "\n",
        "for name, pipeline in pipelines.items():\n",
        "    if (name in pipelines_to_tune.keys()): # Check if the model is in the pipelines_to_tune list\n",
        "      print(f\"\\nSkipping already tuned model: {name}\")\n",
        "      continue\n",
        "    print(f\"\\nEvaluating untuned model: {name}\")\n",
        "    # Fit the original pipeline on the entire training data before evaluating on test set\n",
        "    #pipeline.set_params(**best_params_for_train_untuned_model_dict)\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    model_tuned[name][\"tuned_estimators\"] = pipeline\n",
        "    model_tuned[name][\"tune_results\"] = evaluate_on_test(pipeline, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxN4yMzER3oQ"
      },
      "source": [
        "# Step 6: Model evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS8zXwQsNUh1"
      },
      "source": [
        "## Ranking for each ML models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RItf6cI0Lgp5"
      },
      "outputs": [],
      "source": [
        "# Create a list of dictionaries for the comparison DataFrame\n",
        "comparison_data = []\n",
        "display_metrics_order = [\n",
        "    \"f1_macro\", \"f1_weighted\", \"balanced_acc\", \"average_precision_macro\",       # ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏±‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
        "    \"recall_macro\", \"recall_weighted\", \"precision_macro\", \"precision_weighted\"  # ‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡∏™‡∏°‡∏î‡∏∏‡∏• P/R\n",
        "    \"mcc\",                                                   # ‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏£‡∏ß‡∏°\n",
        "    \"roc_auc_ovr\", \"roc_auc_ovr_macro\", \"log_loss\",          # ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á/‡πÇ‡∏û‡∏£‡∏ö‡∏≤\n",
        "    \"accuracy\"                                               # ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö\n",
        "]\n",
        "\n",
        "for name, vals in model_tuned.items():\n",
        "    row = {\"model\": name}\n",
        "    print(f\"model: \", name)\n",
        "    # Check if 'tune_results' contains tuning results or test metrics\n",
        "    if \"best_score\" in vals[\"tune_results\"]:\n",
        "        # This is a tuned model, get test metrics from evaluate_on_test\n",
        "        test_metrics = evaluate_on_test(vals[\"tuned_estimators\"], X_test, y_test)\n",
        "        row.update(test_metrics)\n",
        "        row[\"model\"] = f\"{name} (Tuned)\" # Add (Tuned) to the name\n",
        "    else:\n",
        "        # This is an original model, test metrics are already in tune_results\n",
        "        print(\"|-> This is an original model, test metrics are already in tune_results\")\n",
        "        row.update(vals[\"tune_results\"])\n",
        "        row[\"model\"] = f\"{name} (Original)\" # Add (Original) to the name\n",
        "\n",
        "    comparison_data.append(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oFKq1EpQnn1"
      },
      "outputs": [],
      "source": [
        "# Select and reorder columns for display\n",
        "# Create the comparison DataFrame\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# Select and reorder columns for display\n",
        "comparison_df = comparison_df[['model'] + [m for m in display_metrics_order if m in comparison_df.columns]].sort_values(\"f1_macro\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Sort by f1_macro, a metric suitable for class imbalance\n",
        "display(comparison_df)\n",
        "\n",
        "# Select the best model based on the highest f1_macro score\n",
        "best_model_name = comparison_df.loc[0, \"model\"]\n",
        "print(f\"\\nBest Model based on f1_macro: {best_model_name}\")\n",
        "\n",
        "pprint(model_tuned[best_model_name.split()[0]][\"tuned_estimators\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wiB4s4NeNRyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export Models"
      ],
      "metadata": {
        "id": "EJfVfDrPYNhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Get the best model name from the comparison DataFrame\n",
        "best_model_name_key = best_model_name.split()[0] # Get the original model name key\n",
        "\n",
        "# Retrieve the best estimator from the model_tuned dictionary\n",
        "best_estimator = model_tuned[best_model_name_key][\"tuned_estimators\"]\n",
        "\n",
        "# Define the filename for saving the model\n",
        "model_filename = f\"{best_model_name_key}_model.joblib\"\n",
        "\n",
        "# Save the best estimator to the file\n",
        "joblib.dump(best_estimator, model_filename)\n",
        "\n",
        "print(f\"Best model '{best_model_name_key}' saved to '{model_filename}'\")\n",
        "print(\"\\nModel Estimators: \")\n",
        "pprint(best_estimator)\n",
        "\n",
        "# Optional: Verify the file exists\n",
        "if os.path.exists(model_filename):\n",
        "    print(f\"\\nFile '{model_filename}' successfully created.\")"
      ],
      "metadata": {
        "id": "vt9q6Y19XEB-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}