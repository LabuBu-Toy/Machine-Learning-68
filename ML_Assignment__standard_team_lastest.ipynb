{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LabuBu-Toy/Machine-Learning-68/blob/main/ML_Assignment__standard_team_lastest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w24HpG8cbRIT"
      },
      "source": [
        "# Fetal-health-classification üë∂üèªüê£üçº\n",
        "\n",
        "Dataset Link: https://www.kaggle.com/datasets/andrewmvd/fetal-health-classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0c5bff5"
      },
      "source": [
        "!pip install imblearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, balanced_accuracy_score,\n",
        "    precision_score, recall_score, f1_score,\n",
        "    log_loss, roc_auc_score\n",
        ")\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
        "from sklearn.model_selection import learning_curve, validation_curve, LearningCurveDisplay, ValidationCurveDisplay\n",
        "from imblearn.metrics import classification_report_imbalanced"
      ],
      "metadata": {
        "id": "srRu630i2pON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0m2vADVgstL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display\n",
        "from pprint import pprint\n",
        "from fnmatch import fnmatchcase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brdEfLXNFcJn"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore') # Ignore any warning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itSPugBCg2t8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report,  log_loss,\n",
        "    confusion_matrix, ConfusionMatrixDisplay, recall_score,\n",
        "    balanced_accuracy_score, make_scorer, precision_score,\n",
        "    f1_score, roc_auc_score, average_precision_score)\n",
        "\n",
        "from imblearn.metrics import classification_report_imbalanced\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eldeIViff5Lw"
      },
      "outputs": [],
      "source": [
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, SVMSMOTE, SMOTEN, SMOTENC, KMeansSMOTE\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.under_sampling import TomekLinks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_PC3FM6gqag"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.calibration import CalibratedClassifierCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4FZm-kMQro-"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import clone\n",
        "from fnmatch import fnmatchcase\n",
        "from collections import Counter, defaultdict\n",
        "from math import ceil\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Optional, Iterable\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.naive_bayes import CategoricalNB\n",
        "from sklearn.model_selection import GridSearchCV, HalvingGridSearchCV\n",
        "\n",
        "from imblearn.over_sampling import (\n",
        "    RandomOverSampler, SMOTE, BorderlineSMOTE, SVMSMOTE,\n",
        "    SMOTEN, SMOTENC, KMeansSMOTE\n",
        ")\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier, RUSBoostClassifier"
      ],
      "metadata": {
        "id": "EFDY98OGLIk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-73R9m1JF1Jb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import HalvingRandomSearchCV\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNJt5CWthElC"
      },
      "outputs": [],
      "source": [
        "sns.set(style='darkgrid')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, f1_score, balanced_accuracy_score, confusion_matrix\n",
        "from scipy.stats import randint, loguniform"
      ],
      "metadata": {
        "id": "kllz3VX5LPiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cf28Jp0OYbcJ"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "from pathlib import Path\n",
        "\n",
        "# Download Dataset from Kaggle\n",
        "dataset_path = kagglehub.dataset_download(\"andrewmvd/fetal-health-classification\")\n",
        "\n",
        "file_path = Path(dataset_path)\n",
        "file_path = file_path / 'fetal_health.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYTUOgUxowME"
      },
      "outputs": [],
      "source": [
        "# Set Random SEED\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wn9kI1vqGSB"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(file_path)\n",
        "display(data)\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"‡∏Ç‡∏ô‡∏≤‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏ö:\", data.shape)\n",
        "data = data.drop_duplicates(keep=\"first\")\n",
        "print(\"‡∏Ç‡∏ô‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏á‡∏•‡∏ö:\", data.shape)"
      ],
      "metadata": {
        "id": "b6EU9rLLiQqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oI92BS1ASeE"
      },
      "outputs": [],
      "source": [
        "pprint(f\"Data Shape: {data.shape}\")\n",
        "pprint(f\"Missing values: {data.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Percentage per class\n",
        "pprint(data['fetal_health'].value_counts(normalize=True, sort=True)\n",
        "     .mul(100).round(4))"
      ],
      "metadata": {
        "id": "xlTJDRZds4s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZp1B9FRx1-B"
      },
      "outputs": [],
      "source": [
        "# Class distribution\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.countplot(x='fetal_health', data=data, palette='Greens')\n",
        "plt.title('Fetal Health Class Distribution')\n",
        "plt.xlabel('Class (1: Normal, 2: Suspect, 3: Pathological)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XHAGseuQaNG"
      },
      "outputs": [],
      "source": [
        "# Counts per class (sorted by feature label)\n",
        "pprint(data['fetal_health'].value_counts(sort=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp50bExpRkEM"
      },
      "outputs": [],
      "source": [
        "# Percentage per class\n",
        "pprint(data['fetal_health'].value_counts(normalize=True, sort=True)\n",
        "     .mul(100).round(4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all feature columns except the target variable\n",
        "feature_columns = data.columns.drop('fetal_health')\n",
        "\n",
        "# Plot histograms for all feature columns, colored by fetal_health\n",
        "for feature in feature_columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(data=data, x=feature, hue='fetal_health', multiple='stack', kde=True, palette='viridis')\n",
        "    plt.title(f'Distribution of {feature} by Fetal Health')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Count')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "DEyaO9O8R-eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzK5JiAudDrQ"
      },
      "outputs": [],
      "source": [
        "# Correlation heatmap pearson method\n",
        "plt.figure(figsize=(15, 12))\n",
        "sns.heatmap(data.corr(method=\"pearson\"), annot=True, fmt='.2f', cmap=\"coolwarm\", square=True) #use\n",
        "plt.title('Feature Correlation Heatmap pearson method')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INpAwNc9y1As"
      },
      "outputs": [],
      "source": [
        "# Correlation heatmap Spearman\n",
        "plt.figure(figsize=(15, 12))\n",
        "sns.heatmap(data.corr(method=\"spearman\"), annot=True, fmt='.2f', cmap=\"coolwarm\", square=True) #use\n",
        "plt.title('Feature Correlation Heatmap spearman method')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e_-Bp_zQXEH"
      },
      "outputs": [],
      "source": [
        "# Features / Target\n",
        "X = data.drop(columns=['fetal_health']).select_dtypes(include=[np.number]).copy()   # explicit list + copy for safety\n",
        "\n",
        "# Optional: drop all columns starting with \"histogram\" (case-insensitive)\n",
        "# X = X.loc[:, ~X.columns.str.lower().str.startswith('histogram')]\n",
        "\n",
        "# Kaggle labels are floats (1.0/2.0/3.0); casting to int is often nicer\n",
        "y = data['fetal_health'].astype('int64')\n",
        "\n",
        "# 2) Check Data Shape\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "assert 'fetal_health' not in X.columns\n",
        "# assert not any(X.columns.str.lower().str.startswith('histogram'))  # if you dropped them"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,          # preserves class proportions in each split\n",
        "    random_state=SEED\n",
        ")"
      ],
      "metadata": {
        "id": "Z7KN8rEvyGfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zii8qhrkInyg"
      },
      "outputs": [],
      "source": [
        "def grid_for(model_name: str, grid_map: dict) -> dict | None:\n",
        "    # allow both exact and wildcard matches; also fix \"LogReg_*\" -> \"LogReg*\"\n",
        "    grid_map = {k.replace(\"_*\", \"*\"): v for k, v in grid_map.items()}\n",
        "    if model_name in grid_map:\n",
        "        print(f\"=> return grid {grid_map[model_name]}\")\n",
        "        return grid_map[model_name]\n",
        "    for pattern, grid in grid_map.items():\n",
        "        if fnmatchcase(model_name, pattern):\n",
        "            print(f\"=> return grid {grid}\")\n",
        "            return grid\n",
        "    print(f\"=> no grid found for {model_name}\")\n",
        "    return None\n",
        "\n",
        "def evaluate_on_test(est, X_test = X_test, y_test = y_test):\n",
        "    \"\"\"Return dict of test metrics and show confusion matrices.\"\"\"\n",
        "\n",
        "    y_pred = est.predict(X_test)\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"balanced_acc\": balanced_accuracy_score(y_test, y_pred),\n",
        "        \"precision_macro\": precision_score(y_test, y_pred, average=\"macro\", zero_division=0),\n",
        "        \"precision_weighted\": precision_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
        "        \"recall_macro\": recall_score(y_test, y_pred, average=\"macro\", zero_division=0),\n",
        "        \"recall_weighted\": recall_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
        "        \"f1_macro\": f1_score(y_test, y_pred, average=\"macro\"),\n",
        "        \"f1_weighted\": f1_score(y_test, y_pred, average=\"weighted\"),\n",
        "    }\n",
        "\n",
        "    # log loss & ROC-AUC if proba/decision available\n",
        "    if hasattr(est, \"predict_proba\"):\n",
        "        y_prob = est.predict_proba(X_test)\n",
        "        try:\n",
        "            metrics[\"log_loss\"] = log_loss(y_test, y_prob, labels=np.unique(y_test))\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            metrics[\"roc_auc_ovr\"] = roc_auc_score(y_test, y_prob, multi_class=\"ovr\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # after fit: oob_score_ exists only if oob_score=True and bootstrap=True\n",
        "    # Try OOB if applicable\n",
        "    try:\n",
        "        metrics[\"oob_score\"] = est.named_steps[\"clf\"].oob_score_  # if pipeline & step named \"clf\"\n",
        "    except Exception:\n",
        "        try:\n",
        "            metrics[\"oob_score\"] = est.oob_score_\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"#\" * 20)\n",
        "\n",
        "    print(\"\\n- Imbalance Classification report (test):\")\n",
        "    print(classification_report_imbalanced(y_test, y_pred, digits=6, zero_division=0))\n",
        "\n",
        "    # Confusion matrices\n",
        "    ConfusionMatrixDisplay.from_estimator(est, X_test, y_test)\n",
        "    plt.title(\"Confusion Matrix (counts)\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"#\" * 20)\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMrOksAwz5YM"
      },
      "source": [
        "## Cross Validation\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wF_95XSopluQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before Tuning"
      ],
      "metadata": {
        "id": "Rhf98054tjBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_notune = ImbPipeline(steps=[\n",
        "    (\"clf\", RandomForestClassifier(\n",
        "        n_jobs=-1,\n",
        "        criterion=\"entropy\",\n",
        "        class_weight=\"balanced_subsample\",\n",
        "        bootstrap=True,\n",
        "        oob_score=True,\n",
        "        random_state=SEED,\n",
        "    )),\n",
        "])\n",
        "\n",
        "# Print the parameters of the untuned pipeline\n",
        "print(\"Parameters of the Untuned Pipeline:\")\n",
        "pprint(pipe_notune.get_params())\n",
        "\n",
        "# Perform cross-validation\n",
        "print(\"\\nPerforming Cross-Validation (Before Tuning):\")\n",
        "cv_results = cross_validate(\n",
        "    pipe_notune,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    cv=cv,\n",
        "    scoring=\"f1_macro\",\n",
        "    n_jobs=-1,\n",
        "    return_estimator=True,\n",
        "    error_score=np.nan,\n",
        ")\n",
        "\n",
        "# Print cross-validation results\n",
        "print(\"\\nCross-Validation Results (Before Tuning):\")\n",
        "for metric, values in cv_results.items():\n",
        "    if 'test_' in metric:\n",
        "        print(f\"{metric}: {np.mean(values):.4f} +/- {np.std(values):.4f}\")\n",
        "\n",
        "# Fit the model on the entire training data\n",
        "print(\"\\nFitting model on entire training data (Before Tuning)...\")\n",
        "pipe_notune.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on the test set\n",
        "print(\"\\nEvaluating model on test set (Before Tuning):\")\n",
        "test_metrics_notune = evaluate_on_test(pipe_notune, X_test, y_test)\n",
        "\n",
        "# Display test scores in a table\n",
        "print(\"\\nSummary of Scores for Untuned Model on Test Set:\")\n",
        "untuned_scores_df = pd.DataFrame([test_metrics_notune])\n",
        "display(untuned_scores_df)"
      ],
      "metadata": {
        "id": "aHuAbnxXtirr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = pipe_notune.named_steps[\"clf\"]\n",
        "\n",
        "# ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠ feature (‡∏™‡∏°‡∏°‡∏ï‡∏¥ X_train ‡πÄ‡∏õ‡πá‡∏ô DataFrame)\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö\n",
        "feat_imp_df = pd.DataFrame({\n",
        "    \"feature\": feature_names,\n",
        "    \"importance\": importances\n",
        "})\n",
        "\n",
        "feat_imp_df = feat_imp_df.sort_values(by=\"importance\", ascending=False)\n",
        "\n",
        "# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• Top 10\n",
        "print(feat_imp_df)\n",
        "\n",
        "# Plot bar chart\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.barh(feat_imp_df[\"feature\"][::-1],\n",
        "         feat_imp_df[\"importance\"][::-1],\n",
        "         color=\"skyblue\")\n",
        "plt.xlabel(\"Relative Importance\")\n",
        "plt.title(\"Feature Importances ‚Äî Random Forest\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oCGZdUVZXhTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7Tq9pTWzFEq"
      },
      "outputs": [],
      "source": [
        "pipelines = {\n",
        "    \"RF_balancedRF\": ImbPipeline([\n",
        "        (\"clf\", BalancedRandomForestClassifier(\n",
        "            bootstrap=True,\n",
        "            oob_score=True,\n",
        "            criterion=\"entropy\",\n",
        "            random_state=SEED\n",
        "        ))\n",
        "    ]),\n",
        "    \"RF_class_weighted\" : ImbPipeline([\n",
        "      (\"clf\", RandomForestClassifier(\n",
        "          bootstrap=True,\n",
        "          oob_score=True,\n",
        "          criterion=\"entropy\",\n",
        "          class_weight=\"balanced_subsample\",\n",
        "          random_state=SEED))\n",
        "    ]),\n",
        "    \"RF_BorderlineSMOTE\": ImbPipeline([\n",
        "        (\"scale\", StandardScaler()),\n",
        "        (\"borderline_smote\", BorderlineSMOTE(random_state=SEED)),\n",
        "        (\"clf\", RandomForestClassifier(\n",
        "            bootstrap=True,\n",
        "            oob_score=True,\n",
        "            criterion=\"entropy\",\n",
        "            random_state=SEED))\n",
        "    ]),\n",
        "    \"RF_SMOTE\": ImbPipeline([\n",
        "        (\"scale\", StandardScaler()),\n",
        "        (\"smote\", SMOTE(random_state=SEED)),\n",
        "        (\"clf\", RandomForestClassifier(\n",
        "            bootstrap=True,\n",
        "            oob_score=True,\n",
        "            criterion=\"entropy\",\n",
        "            random_state=SEED))\n",
        "    ]),\n",
        "    \"RF_SMOTETomek\": ImbPipeline([\n",
        "        (\"scale\", StandardScaler()),\n",
        "        (\"smtomek\", SMOTETomek(random_state=SEED)),\n",
        "        (\"clf\", RandomForestClassifier(\n",
        "            bootstrap=True,\n",
        "            oob_score=True,\n",
        "            criterion=\"entropy\",\n",
        "            random_state=SEED))\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sYH40GCF6jH"
      },
      "source": [
        "## Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFzN0GJzIi88"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import randint, loguniform, uniform\n",
        "\n",
        "search_spaces = {\n",
        "    # === Baselines ===\n",
        "    \"RF_*\": {\n",
        "        \"clf__n_estimators\": [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
        "        \"clf__max_depth\": [None, 10, 20 ,30 ,40, 50, 70, 100, 150, 200],\n",
        "        \"clf__min_samples_split\": [2, 4 ,5 ,8, 10, 20, 50, 100],\n",
        "        \"clf__min_samples_leaf\": [1, 2, 4 ,5 ,8, 10, 20, 50],\n",
        "        \"clf__max_features\": [\"sqrt\", \"log2\", 0.3, 0.5, 0.7, 0.9, 1.0],\n",
        "        \"clf__n_jobs\": [-1],\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xphsqbkJ3o9U"
      },
      "source": [
        "## Tuning Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5UKFWvUL9mV"
      },
      "outputs": [],
      "source": [
        "model_tuned: dict[str, dict] = defaultdict(dict)\n",
        "\n",
        "for model_name, base_est in pipelines.items():\n",
        "    grid = grid_for(model_name, search_spaces)\n",
        "\n",
        "    est = clone(base_est)\n",
        "\n",
        "    print(f\"[TUNE] {model_name} with params: {list(grid.keys()) or '[no tunable params]'}\")\n",
        "\n",
        "    search = HalvingRandomSearchCV(\n",
        "        estimator=est,\n",
        "        param_distributions=grid,\n",
        "        scoring=\"f1_macro\",\n",
        "        cv=cv,\n",
        "        n_jobs=-1,\n",
        "        error_score=np.nan,   # don't bomb entire run on a single bad fold\n",
        "        refit=True,\n",
        "        return_train_score=True,\n",
        "        random_state=SEED,\n",
        "        aggressive_elimination=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Perform the hyperparameter search by fitting the search object\n",
        "    search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "    # Store the tuning results and the best estimator directly\n",
        "    model_tuned[model_name][\"tune_results\"] = {\n",
        "        \"best_score\": float(search.best_score_),\n",
        "        \"best_params\": search.best_params_,\n",
        "    }\n",
        "    model_tuned[model_name][\"tuned_estimators\"] = search.best_estimator_\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report Models"
      ],
      "metadata": {
        "id": "EYzZb2163UWy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CI4XjG65Zod"
      },
      "source": [
        "## Summary Tune Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsTzKJTa5ci0"
      },
      "outputs": [],
      "source": [
        "summary_df = (\n",
        "    pd.DataFrame([\n",
        "        {\"model\": name, **vals[\"tune_results\"]} for name, vals in model_tuned.items()\n",
        "    ]).sort_values(\"best_score\", ascending=False).reset_index(drop=True)\n",
        ")\n",
        "# Rename the 'best_score' column to be more explicit\n",
        "summary_df.rename(columns={\"best_score\": \"best_f1_macro_CV_score\"}, inplace=True)\n",
        "\n",
        "display(summary_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxN4yMzER3oQ"
      },
      "source": [
        "# Step 6: Model evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS8zXwQsNUh1"
      },
      "source": [
        "## Ranking for each ML models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RItf6cI0Lgp5"
      },
      "outputs": [],
      "source": [
        "# Create a list of dictionaries for the comparison DataFrame\n",
        "comparison_data = []\n",
        "\n",
        "for name, vals in model_tuned.items():\n",
        "    row = {\"model\": name}\n",
        "    print(f\"model: \", name)\n",
        "\n",
        "    # Check if 'tune_results' contains tuning results or test metrics\n",
        "    if \"best_score\" in vals[\"tune_results\"]:\n",
        "        # This is a tuned model, get test metrics from evaluate_on_test\n",
        "        test_metrics = evaluate_on_test(vals[\"tuned_estimators\"], X_test, y_test)\n",
        "        row.update(test_metrics)\n",
        "        row[\"model\"] = f\"{name} (Tuned)\" # Add (Tuned) to the name\n",
        "\n",
        "    comparison_data.append(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Models"
      ],
      "metadata": {
        "id": "IkFNAUAN1TZf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oFKq1EpQnn1"
      },
      "outputs": [],
      "source": [
        "# Create the comparison DataFrame\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# Sort by f1_macro, a metric suitable for class imbalance\n",
        "comparison_df = comparison_df.sort_values(\"oob_score\", ascending=False).reset_index(drop=True) # Sort in descending order for correct ranking\n",
        "display(comparison_df)\n",
        "\n",
        "# Select the best model based on the highest f1_macro score\n",
        "best_model_name = comparison_df.loc[0, \"model\"]\n",
        "print(f\"\\nBest Model based on f1_macro: {best_model_name}\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# Display the requested metrics for the best model in a table\n",
        "best_model_metrics = comparison_df.loc[0, ['model', 'oob_score', 'f1_macro', 'balanced_acc', 'roc_auc_ovr']].to_frame().T\n",
        "print(f\"\\nMetrics for the Best Model ({best_model_name}):\")\n",
        "display(best_model_metrics)\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "pprint(model_tuned[best_model_name.split()[0]][\"tuned_estimators\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Selection (ANOVA)"
      ],
      "metadata": {
        "id": "hHvqN86O8Hch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score, precision_score\n",
        "\n",
        "# Using imbalanced-learn pipeline\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import BorderlineSMOTE\n",
        "\n",
        "# Step 1: Fit the Selector ONLY on the Training Data\n",
        "# We use k='all' to get scores for every feature, then we'll filter manually.\n",
        "selector = SelectKBest(score_func=f_classif, k='all')\n",
        "selector.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Step 2: Analyze Scores and Select Features based on a p-value\n",
        "# Create a DataFrame to hold the feature scores for easy interpretation.\n",
        "anova_df = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'F-value': selector.scores_,\n",
        "    'p-value': selector.pvalues_\n",
        "})\n",
        "anova_df = anova_df.sort_values('F-value', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Define our significance level (alpha)\n",
        "P_VALUE_THRESHOLD = 0.1\n",
        "selected_features = anova_df[anova_df['p-value'] < P_VALUE_THRESHOLD]['Feature'].tolist()\n",
        "\n",
        "print(f\"Found {len(selected_features)} features with p-value < {P_VALUE_THRESHOLD}\")\n",
        "print(\"Selected features list:\")\n",
        "pprint(selected_features)\n",
        "print(\"\\nANOVA F-test Results (from training data):\")\n",
        "display(anova_df)\n",
        "\n",
        "\n",
        "# Step 3: Visualize Feature Importance\n",
        "# This plot helps to visually confirm the ranking of features.\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(x=anova_df['F-value'], y=anova_df['Feature'], palette='viridis')\n",
        "plt.title('Feature Importance based on ANOVA F-test (from Training Data)')\n",
        "plt.xlabel('F-value (Higher = More Important)')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Step 4: Create New Datasets with Only Selected Features\n",
        "# We use the 'selected_features' list to filter the columns in both train and test sets.\n",
        "X_train_selected = X_train[selected_features]\n",
        "X_test_selected = X_test[selected_features]\n",
        "\n",
        "print(\"\\nShape of data after feature selection:\")\n",
        "print(\"X_train_selected:\", X_train_selected.shape)\n",
        "print(\"X_test_selected:\", X_test_selected.shape)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# Step 5: Define and Train the ML Pipeline\n",
        "# Create a new pipeline with the same steps as the best tuned model\n",
        "best_tuned_pipeline_template = model_tuned[best_model_name.split()[0]][\"tuned_estimators\"]\n",
        "\n",
        "best_model_anova = ImbPipeline(steps=best_tuned_pipeline_template.steps)\n",
        "\n",
        "# Set the parameters from the best tuned model\n",
        "best_model_anova.set_params(**model_tuned[best_model_name.split()[0]][\"tune_results\"][\"best_params\"])\n",
        "\n",
        "print(\"\\nTraining the model on selected features...\")\n",
        "best_model_anova.fit(X_train_selected, y_train)\n",
        "print(\"Training complete.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# Get and display the final scores\n",
        "test_metrics = evaluate_on_test(best_model_anova, X_test_selected, y_test)\n",
        "scores_df = pd.DataFrame([test_metrics])\n",
        "\n",
        "print(\"\\nSummary of Scores for Model on Test Set:\")\n",
        "display(scores_df)"
      ],
      "metadata": {
        "id": "NkJcFuB4iuE0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "V5E1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}